{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><small><i>\n",
    "All the IPython Notebooks in this **Python Decision Tree and Random Forest** series by Dr. Milaan Parmar are available @ **[GitHub](https://github.com/milaan9/Python_Decision_Tree_and_Random_Forest)**\n",
    "</i></small></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "A Decision Tree is one of the popular and powerful machine learning algorithms that I have learned. It is a non-parametric supervised learning method that can be used for both classification and regression tasks. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. For a classification model, the target values are discrete in nature, whereas, for a regression model, the target values are represented by continuous values. Unlike the black box type of algorithms such as Neural Network, Decision Trees are comparably easier to understand because it shares internal decision-making logic (you will find details in the following session).\n",
    "\n",
    "Despite the fact that many data scientists believe it’s an old method and they may have some doubts of its accuracy due to an overfitting problem, the more recent tree-based models, for example, Random forest (bagging method), gradient boosting (boosting method) and XGBoost (boosting method) are built on the top of decision tree algorithm. Therefore, the concepts and algorithms behind Decision Trees are strongly worth understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are *four* popular types of decision tree algorithms: \n",
    "\n",
    "1. **ID3**\n",
    "2. **CART (Classification And Regression Trees)**\n",
    "3. **Chi-Square**\n",
    "4. **Reduction in Variance**\n",
    "\n",
    "In this class, we'll focus only on the classification trees and the explanation of **ID3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    ">You play golf every Sunday and you invite your best friend, Arthur to come with you every time. Arthur sometimes comes to join but sometimes not. For him, it depends on a number of factors for example, **Weather**, **Temperature**, **Humidity** and **Wind**. We'll use the dataset of last two week to predict whether or not Arthur will join you to play golf. An intuitive way to do this is through a Decision Tree.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/playgolf.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/dt0.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Root Node:** \n",
    "    - The attribute that best classifies the training data, use this attribute at the root of the tree. \n",
    "    - The first split which decides the entire population or sample data should further get divided into two or more homogeneous sets.\n",
    "    \n",
    "* **Splitting:** It is a process of dividing a node into two or more *sub-nodes*.\n",
    "\n",
    ">**Question:** Base on which attribute (feature) to split? What is the best split?\n",
    "\n",
    ">**Answer:** Use the attribute with the highest **Information Gain** or **Gini Gain**\n",
    "\n",
    "* **Decision Node:** This node decides whether/when a *sub-node* splits into further sub-nodes or not.\n",
    "\n",
    "* **Leaf:** Terminal Node that predicts the outcome (categorical or continues value). The *coloured nodes*, i.e., *Yes* and *No* nodes, are the leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 (Iterative Dichotomiser)\n",
    "\n",
    "ID3 decision tree algorithm uses **Information Gain** to decide the splitting points. In order to measure how much information we gain, we can use **Entropy** to calculate the homogeneity of a sample.\n",
    "\n",
    ">**Question:** What is **“Entropy”**? and What is its function?\n",
    "\n",
    ">**Answer:** It is a measure of the amount of uncertainty in a data set. Entropy controls how a Decision Tree decides to split the data. It actually affects how a Decision Tree draws its boundaries.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/ID3.png\" width=\"650\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize the ID3 algorithm as illustrated below:\n",
    "\n",
    "1. Compute the entropy for data-set **Entropy(s)**\n",
    "    - Calculate **Entropy** (Amount of uncertainity in dataset):\n",
    "$$Entropy(S) = \\frac{-p}{p+n}log_{2}(\\frac{p}{p+n}) - \\frac{n}{p+n}log_{2}(\\frac{n}{p+n})$$\n",
    "    \n",
    "2. For every attribute/feature:\n",
    "    - Calculate entropy for all other values **Entropy(A)**\n",
    "             \n",
    "    - Take **Average Information Entropy** for the current attribute\n",
    "        - Calculate **Average Information**:\n",
    "    $$I(Attribute) = \\sum\\frac{p_{i}+n_{i}}{p+n}Entropy(A)$$  \n",
    "    \n",
    "    - Calculate **Gain** for the current attribute\n",
    "        - Calculate **Information Gain**: (Difference in Entropy before and after splitting dataset on attribute A)        \n",
    "$$Gain = Entropy(S) - I(Attribute)$$\n",
    "\n",
    "3. Pick the **Highest Gain Attribute**.\n",
    "4. **Repeat** until we get the tree we desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Calculate the Entropy for dataset Entropy(s)\n",
    "\n",
    "We need to calculate the entropy first. Decision column consists of 14 instances and includes two labels: **Yes** and **No**. There are 9 decisions labeled **Yes**, and 5 decisions labeled **No**.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/ID3pg.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Entropy(S):\n",
    "\n",
    "$$Entropy(S) = (Yes)log_{2}(Yes) - (No)log_{2}(No)$$\n",
    "\n",
    "➡$$Entropy(S) = \\frac{-p}{p+n}log_{2}(\\frac{p}{p+n}) - \\frac{n}{p+n}log_{2}(\\frac{n}{p+n})$$\n",
    "\n",
    "➡$$Entropy(S) = \\frac{-9}{9+5}log_{2}(\\frac{9}{9+5}) - \\frac{5}{9+5}log_{2}(\\frac{5}{9+5})$$\n",
    "\n",
    "➡$$Entropy(S) = \\frac{-9}{14}log_{2}(\\frac{9}{14}) - \\frac{5}{14}log_{2}(\\frac{5}{14})$$\n",
    "\n",
    "➡$$Entropy(S) = 0.940$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calculate Entropy for each Attribute of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for each Attribute: (let say Outlook)\n",
    "\n",
    "#### Calculate Entropy for each Values, i.e for 'Sunny', 'Rainy' and 'Overcast'.\n",
    "\n",
    "| Outlook   | PlayGolf |   | Outlook   | PlayGolf |   | Outlook      | PlayGolf |\n",
    "|:---------:|:--------:|:---:|:---------:|:--------:|:---:|:------------:|:--------:|\n",
    "| **Sunny** | **No**❌ | \\|  | **Rainy** | **Yes**✅| \\|  | **Overcast** | **Yes**✅|\n",
    "| **Sunny** | **No**❌ | \\|  | **Rainy** | **Yes**✅| \\|  | **Overcast** | **Yes**✅|\n",
    "| **Sunny** | **No**❌ | \\|  | **Rainy** | **No**❌ | \\|  | **Overcast** | **Yes**✅|\n",
    "| **Sunny** | **Yes**✅| \\|  | **Rainy** | **Yes**✅| \\|  | **Overcast** | **Yes**✅|\n",
    "| **Sunny** | **Yes**✅| \\|  | **Rainy** | **No**❌ | \\|  |              |           |\n",
    "\n",
    "1. Calculate Entropy(Outlook='Value'):\n",
    "\n",
    "$$Entropy(S) = \\frac{-p}{p+n}log_{2}(\\frac{p}{p+n}) - \\frac{n}{p+n}log_{2}(\\frac{n}{p+n})$$\n",
    "\n",
    "➡$$E(Outlook = Sunny) = \\frac{-2}{5}log_{2}(\\frac{2}{5}) - \\frac{3}{5}log_{2}(\\frac{3}{5}) = 0.971$$\n",
    "\n",
    "➡$$E(Outlook = Rainy) = \\frac{-3}{5}log_{2}(\\frac{3}{5}) - \\frac{2}{5}log_{2}(\\frac{2}{5}) = 0.971$$\n",
    "\n",
    "➡$$E(Outlook = Overcast) = -1 log_{2}(1) - 0 log_{2}(0) = 0$$\n",
    "\n",
    "|    Outlook   | Yes = $p$ | No = $n$ | Entropy |\n",
    "|:------------ |:-------:|:------:|:-------:|\n",
    "| **Sunny**    |**2**    | **3**  | **0.971** |\n",
    "| **Rainy**    |**3**    | **2**  | **0.971** |\n",
    "| **Overcast** |**4**    | **0**  | **0**     |\n",
    "\n",
    "2. Calculate Average Information Entropy(Outlook='Value'):\n",
    "\n",
    "$$I(Outlook) = \\frac{p_{Sunny}+n_{Sunny}}{p+n}Entropy(Outlook=Sunny) + $$\n",
    "➡$$\\frac{p_{Rainy}+n_{Rainy}}{p+n}Entropy(Outlook=Rainy) + $$\n",
    "➡$$\\frac{p_{Overcast}+n_{Overcast}}{p+n}Entropy(Outlook=Overcast)$$\n",
    "\n",
    "➡$$I(Outlook) = \\frac{3+2}{9+5}*(0.971) + \\frac{2+3}{9+5}*(0.971) + \\frac{4+0}{9+5}*(0)$$\n",
    "\n",
    "$$I(Outlook) = 0.693$$\n",
    "\n",
    "3. Calculate Gain: Outlook\n",
    "\n",
    "$$Gain = Entropy(S) - I(Attribute)$$\n",
    "➡$$Entropy(S) = 0.940$$\n",
    "\n",
    "➡$$Gain(Outlook) = 0.940 - 0.693$$\n",
    "➡$$Gain(Outlook) = 0.247$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for each Attribute: (let say Temperature))\n",
    "\n",
    "#### Calculate Entropy for each Values, i.e for 'Hot', 'Mild' and 'Cool'.\n",
    "\n",
    "| Temperature | PlayGolf |   | Temperature | PlayGolf |   | Temperature | PlayGolf |\n",
    "|:-----------:|:--------:|:---:|:-----------:|:--------:|:---:|:-----------:|:--------:|\n",
    "| **Hot** | **No**❌     | \\|  | **Mild**  | **Yes**✅  | \\|  | **Cool** | **Yes**✅  |\n",
    "| **Hot** | **No**❌     | \\|  | **Mild**  | **No**❌   | \\|  | **Cool** | **No**❌   |\n",
    "| **Hot** | **Yes**✅    | \\|  | **Mild**  | **Yes**✅  | \\|  | **Cool** | **Yes**✅  |\n",
    "| **Hot** | **Yes**✅    | \\|  | **Mild**  | **Yes**✅  | \\|  | **Cool** | **Yes**✅  |\n",
    "|         |               | \\|  | **Mild**  | **Yes**✅  | \\|  |          |          |\n",
    "|         |               | \\|  | **Mild**  | **No**❌   | \\|  |          |          |\n",
    "\n",
    "1. Calculate Entropy(Temperature='Value'):\n",
    "\n",
    "$$Entropy(S) = \\frac{-p}{p+n}log_{2}(\\frac{p}{p+n}) - \\frac{n}{p+n}log_{2}(\\frac{n}{p+n})$$\n",
    "\n",
    "➡$$E(Temperature = Hot) = \\frac{-2}{4}log_{2}(\\frac{2}{4}) - \\frac{2}{6}log_{2}(\\frac{2}{6}) = 1$$\n",
    "\n",
    "➡$$E(Temperature = Mild) = \\frac{-4}{6}log_{2}(\\frac{4}{6}) - \\frac{2}{6}log_{2}(\\frac{2}{5}) = 0.918$$\n",
    "\n",
    "➡$$E(Temperature = Cool) = \\frac{-3}{4}log_{2}(\\frac{-3}{4}) - \\frac{-1}{4}log_{2}(\\frac{-1}{4}) = 0.811$$\n",
    "\n",
    "| Temperature | Yes = $p$ | No = $n$ | Entropy   |\n",
    "|:------------|:---------:|:--------:|:---------:|\n",
    "| **Hot**     | **2**     | **2**    | **1**     |\n",
    "| **Mild**    | **4**     | **2**    | **0.918** |\n",
    "| **Cool**    | **3**     | **1**    | **0.811** |\n",
    "\n",
    "2. Calculate Average Information Entropy(Temperature='Value'):\n",
    "\n",
    "$$I(Temperature) = \\frac{p_{Hot}+n_{Hot}}{p+n}Entropy(Temperature=Hot) + $$\n",
    "➡$$\\frac{p_{Mild}+n_{Mild}}{p+n}Entropy(Temperature=Mild) + $$\n",
    "➡$$\\frac{p_{Cool}+n_{Cool}}{p+n}Entropy(Temperature=Cool)$$\n",
    "\n",
    "➡$$I(Temperature) = \\frac{2+2}{9+5}*(1) + \\frac{4+2}{9+5}*(0.918) + \\frac{3+1}{9+5}*(0.811)$$\n",
    "\n",
    "➡$$I(Temperature) = 0.911$$\n",
    "\n",
    "3. Calculate Gain: Temperature\n",
    "\n",
    "$$Gain = Entropy(S) - I(Attribute)$$\n",
    "➡$$Entropy(S) = 0.940$$\n",
    "\n",
    "➡$$Gain(Temperature) = 0.940 - 0.911$$\n",
    "➡$$Gain(Temperature) = 0.029$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for each Attribute: (let say Humidity))\n",
    "\n",
    "#### Calculate Entropy for each Values, i.e for 'Normal' and 'High'.\n",
    "\n",
    "| Humidity | PlayGolf |   | Humidity | PlayGolf | \n",
    "|:--------:|:--------:|:---:|:--------:|:--------:|\n",
    "| **Normal** | **Yes**✅ | \\|  | **High** | **No**❌  | \n",
    "| **Normal** | **No**❌  | \\|  | **High** | **No**❌  | \n",
    "| **Normal** | **Yes**✅ | \\|  | **High** | **Yes**✅ | \n",
    "| **Normal** | **Yes**✅ | \\|  | **High** | **Yes**✅ | \n",
    "| **Normal** | **Yes**✅ | \\|  | **High** | **No**❌  | \n",
    "| **Normal** | **Yes**✅ | \\|  | **High** | **Yes**✅ | \n",
    "| **Normal** | **Yes**✅ | \\|  | **High** | **No**❌  | \n",
    "\n",
    "1. Calculate Entropy(Humidity='Value'):\n",
    "\n",
    "$$Entropy(S) = \\frac{-p}{p+n}log_{2}(\\frac{p}{p+n}) - \\frac{n}{p+n}log_{2}(\\frac{n}{p+n})$$\n",
    "\n",
    "➡$$E(Humidity = Normal) = \\frac{-3}{7}log_{2}(\\frac{3}{7}) - \\frac{4}{7}log_{2}(\\frac{4}{7}) = 0.985$$\n",
    "\n",
    "➡$$E(Humidity = High) = \\frac{-6}{7}log_{2}(\\frac{6}{7}) - \\frac{1}{7}log_{2}(\\frac{1}{7}) = 0.591$$\n",
    "\n",
    "\n",
    "| Humidity   | Yes = $p$ | No = $n$ | Entropy   |\n",
    "|:-----------|:---------:|:--------:|:---------:|\n",
    "| **Normal** | **3**     | **4**    | **0.985** |\n",
    "| **High**   | **6**     | **1**    | **0.591** |\n",
    "\n",
    "2. Calculate Average Information Entropy(Humidity='Value'):\n",
    "\n",
    "$$I(Humidity) = \\frac{p_{Normal}+n_{Normal}}{p+n}Entropy(Humidity=Normal) + $$\n",
    "➡$$\\frac{p_{High}+n_{High}}{p+n}Entropy(Humidity=High)$$\n",
    "\n",
    "➡$$I(Humidity) = \\frac{3+4}{9+5}*(0.985) + \\frac{6+1}{9+5}*(0.591) $$\n",
    "\n",
    "➡$$I(Humidity) = 0.788$$\n",
    "\n",
    "3. Calculate Gain: Humidity\n",
    "\n",
    "$$Gain = Entropy(S) - I(Attribute)$$\n",
    "➡$$Entropy(S) = 0.940$$\n",
    "\n",
    "➡$$Gain(Humidity) = 0.940 - 0.788$$\n",
    "➡$$Gain(Humidity) = 0.152$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for each Attribute: (let say Wind))\n",
    "\n",
    "#### Calculate Entropy for each Values, i.e for 'Weak' and 'Strong'.\n",
    "\n",
    "| Wind     | PlayGolf |   | Wind     | PlayGolf | \n",
    "|:--------:|:--------:|:---:|:--------:|:--------:|\n",
    "| **Weak** | **No**❌  | \\|  | **Strong** | **No**❌  | \n",
    "| **Weak** | **Yes**✅ | \\|  | **Strong** | **No**❌  | \n",
    "| **Weak** | **Yes**✅ | \\|  | **Strong** | **Yes**✅ | \n",
    "| **Weak** | **Yes**✅ | \\|  | **Strong** | **Yes**✅ | \n",
    "| **Weak** | **No**❌  | \\|  | **Strong** | **Yes**✅ | \n",
    "| **Weak** | **Yes**✅ | \\|  | **Strong** | **No**❌  | \n",
    "| **Weak** | **Yes**✅ | \\|  |            |         | \n",
    "| **Weak** | **Yes**✅ | \\|  |            |         | \n",
    "\n",
    "1. Calculate Entropy(Wind='Value'):\n",
    "\n",
    "$$Entropy(S) = \\frac{-p}{p+n}log_{2}(\\frac{p}{p+n}) - \\frac{n}{p+n}log_{2}(\\frac{n}{p+n})$$\n",
    "\n",
    "➡$$E(Wind = Normal) = \\frac{-6}{8}log_{2}(\\frac{6}{8}) - \\frac{2}{8}log_{2}(\\frac{2}{8}) = 0.811$$\n",
    "\n",
    "➡$$E(Wind = High) = \\frac{-3}{6}log_{2}(\\frac{3}{6}) - \\frac{3}{6}log_{2}(\\frac{3}{6}) = 1$$\n",
    "\n",
    "\n",
    "| Wind    | Yes = $p$ | No = $n$ | Entropy   |\n",
    "|:--------|:---------:|:--------:|:---------:|\n",
    "| **Weak**   | **6**     | **2**    | **0.811** |\n",
    "| **Strong** | **3**     | **3**    | **1**     |\n",
    "\n",
    "2. Calculate Average Information Entropy(Wind='Value'):\n",
    "\n",
    "$$I(Wind) = \\frac{p_{Weak}+n_{Weak}}{p+n}Entropy(Wind=Weak) + $$\n",
    "➡$$\\frac{p_{Strong}+n_{Strong}}{p+n}Entropy(Wind=Strong)$$\n",
    "\n",
    "➡$$I(Wind) = \\frac{6+2}{9+5}*(0.811) + \\frac{3+3}{9+5}*(1) $$\n",
    "\n",
    "➡$$I(Wind) = 0.892$$\n",
    "\n",
    "3. Calculate Gain: Wind\n",
    "\n",
    "$$Gain = Entropy(S) - I(Attribute)$$\n",
    "➡$$Entropy(S) = 0.940$$\n",
    "\n",
    "➡$$Gain(Wind) = 0.940 - 0.892$$\n",
    "➡$$Gain(Wind) = 0.048$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Select Root Node of Dataset\n",
    "\n",
    "Pick the highest Gain attribute.\n",
    "\n",
    "|    Attributes   |    Gain   |           |\n",
    "|:----------------|:---------:|:---------:|\n",
    "| **Outlook**     | **0.247** | ⬅️ Root node|\n",
    "| **Temperature** | **0.029** | |\n",
    "| **Humidity**    | **0.152** | |\n",
    "| **Wind**        | **0.048** | |\n",
    "\n",
    "As seen, **Outlook** factor on decision produces the highest score. That's why, outlook decision will appear in the root node of the tree.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/dt1.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate Entropy for dataset when Outlook is Sunny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to test dataset for custom subsets of Outlook attribute.\n",
    "\n",
    "**Outlook = Overcast**\n",
    "\n",
    "| Outlook | Temperature | Humidity | Windy | PlayGolf |          |\n",
    "|:-------:|:-----------:|:--------:|:-----:|:--------:|:--------:|\n",
    "| **Overcast** | **Hot**  | **High**   | **Weak**   | **Yes**  | ✅ |\n",
    "| **Overcast** | **Cool** | **Normal** | **Strong** | **Yes**  | ✅ |\n",
    "| **Overcast** | **Mild** | **High**   | **Weak**   | **Yes**  | ✅ |\n",
    "| **Overcast** | **Hot**  | **Normal** | **Strong** | **Yes**  | ✅ |\n",
    "\n",
    "Basically, decision will always be **Yes** if outlook were overcast.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/dt2.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply same principles to those sub-trees till we get the tree.\n",
    "\n",
    "Focus on the sub-trees for **Sunny** **Outlook**. We need to find the Gain scores for **Temperature**, **Humidity** and **Wind** attributes respectively.\n",
    "\n",
    "**Outlook = Sunny**\n",
    "\n",
    "| Outlook | Temperature | Humidity | Windy | PlayGolf |          |\n",
    "|:-------:|:-----------:|:--------:|:-----:|:--------:|:--------:|\n",
    "| **Sunny** | **Hot**  | **High**   | **Weak**   | **No**  | ❌ |\n",
    "| **Sunny** | **Hot**  | **High**   | **Strong** | **No**  | ❌ | \n",
    "| **Sunny** | **Mild** | **High**   | **Weak**   | **No**  | ❌ | \n",
    "| **Sunny** | **Cool** | **Normal** | **Weak**   | **Yes** | ✅ | \n",
    "| **Sunny** | **Mild** | **Normal** | **Strong** | **Yes** | ✅ | \n",
    "\n",
    "$$p = 2, n = 3$$\n",
    "\n",
    "Calculate Entropy(S):\n",
    "\n",
    "$$Entropy(S) = (Yes)log_{2}(Yes) - (No)log_{3}(No)$$\n",
    "\n",
    "➡$$Entropy(S) = \\frac{-p}{p+n}log_{2}(\\frac{p}{p+n}) - \\frac{n}{p+n}log_{2}(\\frac{n}{p+n})$$\n",
    "\n",
    "➡$$Entropy(S) = \\frac{-2}{2+3}log_{2}(\\frac{2}{2+3}) - \\frac{3}{2+3}log_{2}(\\frac{3}{2+3})$$\n",
    "\n",
    "➡$$Entropy(S) = 0.971$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate Entropy for each Attribute of Dataset when Outlook is Sunny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for each Attribute: (let say Temperature) for Sunny Outlook\n",
    "\n",
    "#### Calculate Entropy for each Temperature, i.e for Cool', 'Hot' and 'Mild' for Sunny Outlook.\n",
    "\n",
    "| Outlook |  Temperature | PlayGolf |          |\n",
    "|:-------:|--------:|--------:|:--------:|\n",
    "| **Sunny** | **Cool** | **Yes** | ✅ |\n",
    "| **Sunny** | **Hot**  | **No**  | ❌ | \n",
    "| **Sunny** | **Hot**  | **No**  | ❌ | \n",
    "| **Sunny** | **Mild** | **No**  | ❌ | \n",
    "| **Sunny** | **Mild** | **Yes** | ✅ |\n",
    "\n",
    "| Temperature   | Yes = $p$ | No = $n$ | Entropy  |\n",
    "|:--------|:---------:|:--------:|:--------:|\n",
    "| **Cool** | **1**  | **0**    | **0**    |\n",
    "| **Hot**  | **0**  | **2**    | **0**|\n",
    "| **Mild** | **1**  | **1**    | **1**|\n",
    "\n",
    "1. Calculate Average Information Entropy(Outlook=Sunny|Temperature):\n",
    "\n",
    "$$I(Outlook=Sunny|Temperature) = 0.4$$\n",
    "\n",
    "2. Calculate Gain(Outlook=Sunny|Temperature):\n",
    "\n",
    "$$Gain(Outlook=Sunny|Temperature) = 0.571$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for each Attribute: (let say Humidity) for Sunny Outlook\n",
    "\n",
    "#### Calculate Entropy for each Humidity, i.e for 'High' and 'Normal' for Sunny Outlook.\n",
    "\n",
    "| Outlook |  Humidity | PlayGolf |          |\n",
    "|:-------:|--------:|--------:|:--------:|\n",
    "| **Sunny** | **High**   | **No**  | ❌ |\n",
    "| **Sunny** | **High**   | **No**  | ❌ | \n",
    "| **Sunny** | **High**   | **No**  | ❌ | \n",
    "| **Sunny** | **Normal** | **Yes** | ✅ | \n",
    "| **Sunny** | **Normal** | **Yes** | ✅ |\n",
    "\n",
    "| Humidity| Yes = $p$ | No = $n$ | Entropy |\n",
    "|:--------|:---------:|:--------:|:-------:|\n",
    "| **High**   | **0**  | **3**    | **0**   |\n",
    "| **Normal** | **2**  | **0**    | **0**   |\n",
    "\n",
    "1. Calculate Average Information Entropy(Outlook=Sunny|Humidity):\n",
    "\n",
    "$$I(Outlook=Sunny|Humidity) = 0$$\n",
    "\n",
    "2. Calculate Gain(Outlook=Sunny|Humidity):\n",
    "\n",
    "$$Gain(Outlook=Sunny|Humidity) = 0.971$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for each Attribute: (let say Windy) for Sunny Outlook\n",
    "\n",
    "#### Calculate Entropy for each Windy, i.e for 'Strong' and 'Weak' for Sunny Outlook.\n",
    "\n",
    "| Outlook |  Wind  | PlayGolf |          |\n",
    "|:-------:|--------:|--------:|:--------:|\n",
    "| **Sunny** | **Strong** | **No**  | ❌ |\n",
    "| **Sunny** | **Strong** | **Yes** | ✅ | \n",
    "| **Sunny** | **Weak**   | **No**  | ❌ | \n",
    "| **Sunny** | **Weak**   | **No**  | ❌ | \n",
    "| **Sunny** | **Weak**   | **Yes** | ✅ |\n",
    "\n",
    "| Wind    | Yes = $p$ | No = $n$ | Entropy  |\n",
    "|:--------|:---------:|:--------:|:--------:|\n",
    "| **Strong** | **1**  | **1**    | **1**    |\n",
    "| **Weak**   | **1**  | **2**    | **0.918**|\n",
    "\n",
    "1. Calculate Average Information Entropy(Outlook=Sunny|Wind):\n",
    "\n",
    "$$I(Outlook=Sunny|Windy) = 0.951$$\n",
    "\n",
    "2. Calculate Gain(Outlook=Sunny|Wind):\n",
    "\n",
    "$$Gain(Outlook=Sunny|Windy) = 0.020$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Select Root Node of Dataset for Sunny Outlook\n",
    "\n",
    "Pick the highest gain attribute.\n",
    "\n",
    "|    Attributes   |    Gain   |           |\n",
    "|:----------------|:---------:|:---------:|\n",
    "| **Humidity**    | **0.971** | ⬅️ Root node|\n",
    "| **Wind**        | **0.02** | |\n",
    "| **Temperature** | **0.571** | |\n",
    "\n",
    "As seen, **Humidity** factor on decision produces the highest score. That's why, **Humidity** decision will appear in the next node of the Sunny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/dt3.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Calculate Entropy for each Attribute of Dataset when Outlook is Rainy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to focus on **Rainy** **Outlook**.\n",
    "\n",
    "Focus on the sub-tree for **Rainy** **Outlook**. We need to find the Gain scores for **Temperature**, **Humidity** and **Wind** attributes respectively.\n",
    "\n",
    "**Outlook = Rainy**\n",
    "\n",
    "| Outlook | Temperature | Humidity | Wind  | PlayGolf |          |\n",
    "|:-------:|:-----------:|:--------:|:-----:|:--------:|:--------:|\n",
    "| **Rainy** | **Mild**  | **High**   | **Weak**   | **Yes** | ✅ |\n",
    "| **Rainy** | **Cool**  | **Normal** | **Weak**   | **Yes** | ✅ |\n",
    "| **Rainy** | **Cool**  | **Normal** | **Strong** | **No**  | ❌ |\n",
    "| **Rainy** | **Mild**  | **Normal** | **Weak**   | **Yes** | ✅ |\n",
    "| **Rainy** | **Mild**  | **High**   | **Strong** | **No**  | ❌ |\n",
    "\n",
    "$$p = 3, n = 2$$\n",
    "\n",
    "Calculate Entropy(S):\n",
    "\n",
    "$$Entropy(S) = (Yes)log_{2}(Yes) - (No)log_{3}(No)$$\n",
    "\n",
    "➡$$Entropy(S) = \\frac{-p}{p+n}log_{2}(\\frac{p}{p+n}) - \\frac{n}{p+n}log_{2}(\\frac{n}{p+n})$$\n",
    "\n",
    "➡$$Entropy(S) = \\frac{-3}{2+3}log_{2}(\\frac{3}{2+3}) - \\frac{2}{2+3}log_{2}(\\frac{2}{2+3})$$\n",
    "\n",
    "➡$$Entropy(S) = 0.971$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for each Attribute: (let say Temperature) for Sunny Rainy\n",
    "\n",
    "#### Calculate Entropy for each Temperature, i.e for Cool', 'Hot' and 'Mild' for Sunny Rainy.\n",
    "\n",
    "| Outlook |  Temperature | PlayGolf |          |\n",
    "|:-------:|-------------:|---------:|:--------:|\n",
    "| **Rainy** | **Mild** | **Yes** | ✅ |\n",
    "| **Rainy** | **Cool** | **Yes** | ✅ | \n",
    "| **Rainy** | **Cool** | **No**  | ❌ | \n",
    "| **Rainy** | **Mild** | **Yes** | ✅ | \n",
    "| **Rainy** | **Mild** | **No**  | ❌ |\n",
    "\n",
    "| Temperature | Yes = $p$ | No = $n$ | Entropy  |\n",
    "|:------------|:---------:|:--------:|:--------:|\n",
    "| **Cool**  | **1**  | **1**    | **1**    |\n",
    "| **Mild**  | **2**  | **1**    | **0.918**|\n",
    "\n",
    "1. Calculate Average Information Entropy(Outlook=Rainy|Temperature):\n",
    "\n",
    "$$I(Outlook=Rainy|Temperature) = 0.951$$\n",
    "\n",
    "2. Calculate Gain(Outlook=Rainy|Temperature):\n",
    "\n",
    "$$Gain(Outlook=Rainy|Temperature) = 0.02$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for each Attribute: (let say Humidity) for Sunny Rainy\n",
    "\n",
    "#### Calculate Entropy for each Humidity, i.e for 'High' and 'Normal' for Sunny Rainy.\n",
    "\n",
    "| Outlook |  Humidity | PlayGolf |          |\n",
    "|:-------:|----------:|---------:|:--------:|\n",
    "| **Rainy** | **High**   | **Yes** | ✅ |\n",
    "| **Rainy** | **High**   | **No**  | ❌ | \n",
    "| **Rainy** | **Normal** | **Yes** | ✅ | \n",
    "| **Rainy** | **Normal** | **No**  | ❌ | \n",
    "| **Rainy** | **Normal** | **Yes** | ✅ |\n",
    "\n",
    "| Humidity| Yes = $p$ | No = $n$ | Entropy |\n",
    "|:--------|:---------:|:--------:|:-------:|\n",
    "| **High**   | **1**  | **1**    | **1**   |\n",
    "| **Normal** | **2**  | **1**    | **0.918**   |\n",
    "\n",
    "1. Calculate Average Information Entropy(Outlook=Rainy|Humidity):\n",
    "\n",
    "$$I(Outlook=Rainy|Humidity) = 0.951$$\n",
    "\n",
    "2. Calculate Gain(Outlook=Rainy|Humidity):\n",
    "\n",
    "$$Gain(Outlook=Rainy|Humidity)= 0.02$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy for each Attribute: (let say Windy)  for Sunny Rainy\n",
    "\n",
    "#### Calculate Entropy for each Windy, i.e for 'Strong' and 'Weak'  for Sunny Rainy.\n",
    "\n",
    "| Outlook |  Wind   | PlayGolf |          |\n",
    "|:-------:|--------:|---------:|:--------:|\n",
    "| **Rainy** | **Strong** | **No**  | ❌ |\n",
    "| **Rainy** | **Strong** | **No**  | ❌ | \n",
    "| **Rainy** | **Weak**   | **Yes** | ✅ | \n",
    "| **Rainy** | **Weak**   | **Yes** | ✅ | \n",
    "| **Rainy** | **Weak**   | **Yes** | ✅ |\n",
    "\n",
    "| Wind    | Yes = $p$ | No = $n$ | Entropy  |\n",
    "|:--------|:---------:|:--------:|:--------:|\n",
    "| **Strong** | **0**  | **2**    | **0**    |\n",
    "| **Weak**   | **3**  | **0**    | **0**|\n",
    "\n",
    "1. Calculate Average Information Entropy(Outlook=Rainy|Wind):\n",
    "\n",
    "$$I(Outlook=Rainy|Windy) = 0$$\n",
    "\n",
    "2. Calculate Gain(Outlook=Rainy|Wind):\n",
    "\n",
    "$$Gain(Outlook=Rainy|Windy) = 0.971$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Select Root Node of Dataset for Rainy Outlook\n",
    "\n",
    "Pick the highest gain attribute.\n",
    "\n",
    "|    Attributes   |    Gain   |           |\n",
    "|:----------------|:---------:|:---------:|\n",
    "| **Humidity**    | **0.02**  | |\n",
    "| **Windy**       | **0.971** | ⬅️ Root node|\n",
    "| **Temperature** | **0.02**  | |\n",
    "\n",
    "As seen, **Wind** factor on decision produces the highest score. That's why, **Wind** decision will appear in the next node of the Rainy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/dt.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "So, decision tree construction is over. We can use the following rules for decisioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same problem is solved with CART algorithm **[here](https://github.com/milaan9/Python_Decision_Tree_and_Random_Forest/blob/main/002_Decision_Tree_PlayGolf_CART.ipynb)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree \n",
    "\n",
    "Write a program to demonstrate the working of the decision tree based ID3 algorithm. Use an appropriate data set for building the decision tree and apply this knowledge to classify a new sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T09:33:29.231543Z",
     "start_time": "2021-08-23T09:33:29.201274Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset/playgolf_data.csv\")\n",
    "print(\"\\n Given Play Golf Dataset:\\n\\n\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T09:37:48.301842Z",
     "start_time": "2021-08-23T09:37:48.293055Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = df.keys()[-1]\n",
    "print('Target Attribute is   ➡ ', t)\n",
    "\n",
    "# Get the attribute names from input dataset\n",
    "attribute_names = list(df.keys())\n",
    "\n",
    "#Remove the target attribute from the attribute names list\n",
    "attribute_names.remove(t) \n",
    "\n",
    "print('Predicting Attributes ➡ ', attribute_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of the Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T09:52:01.550860Z",
     "start_time": "2021-08-23T09:52:01.529380Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function to calculate the entropy of probaility of observations\n",
    "# -p*log2*p\n",
    "\n",
    "import math\n",
    "def entropy(probs):  \n",
    "    return sum( [-prob*math.log(prob, 2) for prob in probs])\n",
    "\n",
    "#Function to calulate the entropy of the given Datasets/List with respect to target attributes\n",
    "def entropy_of_list(ls,value):  \n",
    "    from collections import Counter\n",
    "    \n",
    "    # Total intances associated with respective attribute\n",
    "    total_instances = len(ls)  # = 14\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(\"\\nTotal no of instances/records associated with '{0}' is ➡ {1}\".format(value,total_instances))\n",
    "    # Counter calculates the propotion of class\n",
    "    cnt = Counter(x for x in ls)\n",
    "    print('\\nTarget attribute class count(Yes/No)=',dict(cnt))\n",
    "    \n",
    "    # x means no of YES/NO\n",
    "    probs = [x / total_instances for x in cnt.values()]  \n",
    "    print(\"\\nClasses➡\", max(cnt), min(cnt))\n",
    "    print(\"\\nProbabilities of Class 'p'='{0}' ➡ {1}\".format(max(cnt),max(probs)))\n",
    "    print(\"Probabilities of Class 'n'='{0}'  ➡ {1}\".format(min(cnt),min(probs)))\n",
    "    \n",
    "    # Call Entropy \n",
    "    return entropy(probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain of Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T09:54:05.764306Z",
     "start_time": "2021-08-23T09:54:05.739898Z"
    }
   },
   "outputs": [],
   "source": [
    "def information_gain(df, split_attribute, target_attribute,battr):\n",
    "    print(\"\\n\\n----- Information Gain Calculation of\",split_attribute,\"----- \") \n",
    "    \n",
    "    # group the data based on attribute values\n",
    "    df_split = df.groupby(split_attribute) \n",
    "    glist=[]\n",
    "    for gname,group in df_split:\n",
    "        print('Grouped Attribute Values \\n',group)\n",
    "        print(\"---------------------------------------------------------\")\n",
    "        glist.append(gname) \n",
    "    \n",
    "    glist.reverse()\n",
    "    nobs = len(df.index) * 1.0   \n",
    "    df_agg1=df_split.agg({target_attribute:lambda x:entropy_of_list(x, glist.pop())})\n",
    "    df_agg2=df_split.agg({target_attribute :lambda x:len(x)/nobs})\n",
    "    \n",
    "    df_agg1.columns=['Entropy']\n",
    "    df_agg2.columns=['Proportion']\n",
    "    \n",
    "    # Calculate Information Gain:\n",
    "    new_entropy = sum( df_agg1['Entropy'] * df_agg2['Proportion'])\n",
    "    if battr !='S':\n",
    "        old_entropy = entropy_of_list(df[target_attribute],'S-'+df.iloc[0][df.columns.get_loc(battr)])\n",
    "    else:\n",
    "        old_entropy = entropy_of_list(df[target_attribute],battr)\n",
    "    return old_entropy - new_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T09:55:48.640028Z",
     "start_time": "2021-08-23T09:55:48.615620Z"
    }
   },
   "outputs": [],
   "source": [
    "def id3(df, target_attribute, attribute_names, default_class=None,default_attr='S'):\n",
    "    \n",
    "    from collections import Counter\n",
    "    cnt = Counter(x for x in df[target_attribute])# class of YES /NO\n",
    "    \n",
    "    ## First check: Is this split of the dataset homogeneous?\n",
    "    if len(cnt) == 1:\n",
    "        return next(iter(cnt))  # next input data set, or raises StopIteration when EOF is hit.\n",
    "    \n",
    "    ## Second check: Is this split of the dataset empty? if yes, return a default value\n",
    "    elif df.empty or (not attribute_names):\n",
    "        return default_class  # Return None for Empty Data Set\n",
    "    \n",
    "    ## Otherwise: This dataset is ready to be devied up!\n",
    "    else:\n",
    "        # Get Default Value for next recursive call of this function:\n",
    "        default_class = max(cnt.keys()) #No of YES and NO Class\n",
    "        # Compute the Information Gain of the attributes:\n",
    "        gainz=[]\n",
    "        for attr in attribute_names:\n",
    "            ig= information_gain(df, attr, target_attribute,default_attr)\n",
    "            gainz.append(ig)\n",
    "            print('\\nInformation gain of','“',attr,'”','is ➡', ig)\n",
    "            print(\"=========================================================\")\n",
    "        \n",
    "        index_of_max = gainz.index(max(gainz))               # Index of Best Attribute\n",
    "        best_attr = attribute_names[index_of_max]            # Choose Best Attribute to split on\n",
    "        print(\"\\nList of Gain for arrtibutes:\",attribute_names,\"\\nare:\", gainz,\"respectively.\")\n",
    "        print(\"\\nAttribute with the maximum gain is ➡\", best_attr)\n",
    "        print(\"\\nHence, the Root node will be ➡\", best_attr)\n",
    "        print(\"=========================================================\")\n",
    "\n",
    "        # Create an empty tree, to be populated in a moment\n",
    "        tree = {best_attr:{}} # Initiate the tree with best attribute as a node \n",
    "        remaining_attribute_names =[i for i in attribute_names if i != best_attr]\n",
    "        \n",
    "        # Split dataset-On each split, recursively call this algorithm.Populate the empty tree with subtrees, which\n",
    "        # are the result of the recursive call\n",
    "        for attr_val, data_subset in df.groupby(best_attr):\n",
    "            subtree = id3(data_subset,target_attribute, remaining_attribute_names,default_class,best_attr)\n",
    "            tree[best_attr][attr_val] = subtree\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T09:55:51.281611Z",
     "start_time": "2021-08-23T09:55:50.905640Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function to calulate the entropy of the given Dataset with respect to target attributes\n",
    "def entropy_dataset(a_list):  \n",
    "    from collections import Counter\n",
    "\n",
    "    # Counter calculates the propotion of class\n",
    "    cnt = Counter(x for x in a_list)   \n",
    "    num_instances = len(a_list)*1.0    # = 14\n",
    "    print(\"\\nNumber of Instances of the Current Sub-Class is {0}\".format(num_instances ))\n",
    "    \n",
    "    # x means no of YES/NO\n",
    "    probs = [x / num_instances for x in cnt.values()]  \n",
    "    print(\"\\nClasses➡\", \"'p'=\",max(cnt), \"'n'=\",min(cnt))\n",
    "    print(\"\\nProbabilities of Class 'p'='{0}' ➡ {1}\".format(max(cnt),max(probs)))\n",
    "    print(\"Probabilities of Class 'n'='{0}'  ➡ {1}\".format(min(cnt),min(probs)))\n",
    "    \n",
    "    # Call Entropy\n",
    "    return entropy(probs) \n",
    "    \n",
    "# The initial entropy of the YES/NO attribute for our dataset.\n",
    "print(\"Entropy calculation for input dataset:\\n\")\n",
    "print(df['PlayGolf'])\n",
    "\n",
    "total_entropy = entropy_dataset(df['PlayGolf'])\n",
    "print(\"\\nTotal Entropy(S) of PlayGolf Dataset➡\", total_entropy)\n",
    "print(\"=========================================================\")\n",
    "####################################################\n",
    "\n",
    "from pprint import pprint\n",
    "tree = id3(df,t,attribute_names)\n",
    "print(\"\\nThe Resultant Decision Tree is: ⤵\\n\")\n",
    "pprint(tree)\n",
    "\n",
    "attribute = next(iter(tree))\n",
    "print(\"\\nBest Attribute ➡\",attribute)\n",
    "print(\"Tree Keys      ➡\",tree[attribute].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T10:06:08.111700Z",
     "start_time": "2021-08-23T10:06:08.077525Z"
    }
   },
   "outputs": [],
   "source": [
    "def classify(instance, tree,default=None):  # Instance of Play Tennis with Predicted    \n",
    "    attribute = next(iter(tree))            # Outlook/Humidity/Wind       \n",
    "    if instance[attribute] in tree[attribute].keys(): # Value of the attributs in  set of Tree keys  \n",
    "        result = tree[attribute][instance[attribute]]\n",
    "        if isinstance(result, dict):                  # this is a tree, delve deeper\n",
    "            return classify(instance, result)\n",
    "        else:\n",
    "            return result # this is a label\n",
    "    else:\n",
    "        return default\n",
    "    \n",
    "df_new=pd.read_csv('dataset/playgolf_test.csv')\n",
    "df_new['Predicted'] = df_new.apply(classify, axis=1, args=(tree,'?')) \n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T06:33:26.866830Z",
     "start_time": "2021-08-23T06:33:26.849254Z"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree using `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T17:51:16.325448Z",
     "start_time": "2021-08-16T17:50:59.256709Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the necessary module!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T13:35:42.975229Z",
     "start_time": "2021-08-18T13:35:42.946908Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing data\n",
    "\n",
    "df = pd.read_csv(\"dataset/playgolf_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T17:51:39.040926Z",
     "start_time": "2021-08-16T17:51:38.952308Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T17:51:39.895942Z",
     "start_time": "2021-08-16T17:51:39.788826Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T17:54:01.743940Z",
     "start_time": "2021-08-16T17:54:01.696226Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting categorical variables into dummies/indicator variables\n",
    "\n",
    "df_getdummy=pd.get_dummies(data=df, columns=['Temperature', 'Humidity', 'Outlook', 'Wind'])\n",
    "df_getdummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T17:54:34.390481Z",
     "start_time": "2021-08-16T17:54:34.376851Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separating the training set and test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_getdummy.drop('PlayGolf',axis=1)\n",
    "y = df_getdummy['PlayGolf']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T17:54:38.020417Z",
     "start_time": "2021-08-16T17:54:37.954687Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing Decision Tree Classifier via sklean\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(criterion='entropy',max_depth=2)\n",
    "dtree.fit(X_train,y_train)\n",
    "predictions = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-16T17:53:24.509209Z",
     "start_time": "2021-08-16T17:53:20.935758Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualising the decision tree diagram\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "a = plot_tree(dtree, feature_names=df_getdummy.columns, fontsize=12, filled=True, \n",
    "              class_names=['Not_Play', 'Play'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to improve the model performance (Hyperparameters Optimization), you should adjust the hyperparameters. For more details, please check out **[here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major disadvantage of Decision Trees is overfitting, especially when a tree is particularly deep. Fortunately, the more recent tree-based models including random forest and XGBoost are built on the top of decision tree algorithm and they generally perform better with a strong modeling technique and much more dynamic than a single decision tree. Therefore, understanding the concepts and algorithms behind Decision Trees thoroughly is super helpful to construct a good foundation of learning data science and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "03e77c7cd302261a18a806af7578cf167306b4c926bd0db4130f26082be6a377"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
